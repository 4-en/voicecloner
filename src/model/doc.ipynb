{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import os # for file path manipulation\n",
    "\n",
    "import csv # for reading tsv files\n",
    "\n",
    "\n",
    "# custom dataset class\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, tsvs=[], sample_rate=16000, transform=None, columns=['path']):\n",
    "        self.tsvs = tsvs\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        self.columns = columns\n",
    "        self.data = []\n",
    "\n",
    "        # load metadata\n",
    "        self._load_metadata()\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        self.data = []\n",
    "        for tsv in self.tsvs:\n",
    "            dir_path, _ = os.path.split(tsv)\n",
    "            \n",
    "            clips = os.path.join(dir_path, 'clips', '')\n",
    "            \n",
    "            # read tsv and append to data\n",
    "            with open(tsv, 'r') as f:\n",
    "                reader = csv.DictReader(f, delimiter='\\t')\n",
    "                for row in reader:\n",
    "                    # commonvoice columns:\n",
    "                    # client_id\tpath\tsentence\tup_votes\tdown_votes\tage\tgender\taccents\tvariant\tlocale\tsegment\n",
    "                    \n",
    "                    # get columns\n",
    "                    data = [row[col] for col in self.columns]\n",
    "                    if 'path' in self.columns:\n",
    "                        # convert path to absolute path\n",
    "                        path_idx = self.columns.index('path')\n",
    "                        data[path_idx] = clips + data[path_idx]\n",
    "                    # append to data\n",
    "                    self.data.append(data)\n",
    "\n",
    "\n",
    "        # shuffle data\n",
    "        np.random.shuffle(self.data)\n",
    "\n",
    "    def get_column_names(self):\n",
    "        # if path is included, last column is audio data that will be loaded in __getitem__\n",
    "        if 'path' in self.columns:\n",
    "            # self.columns + ['audio']\n",
    "            return self.columns + ['audio']\n",
    "        else:\n",
    "            return self.columns\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # load data\n",
    "        sample = self.data[idx]\n",
    "        # load audio (if path is in sample)\n",
    "        if 'path' in self.columns:\n",
    "            # load audio\n",
    "            #print(sample[self.columns.index('path')])\n",
    "            audio, sample_rate = torchaudio.load(sample[self.columns.index('path')])\n",
    "            \n",
    "            # resample audio if necessary\n",
    "            if sample_rate != self.sample_rate:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, self.sample_rate)\n",
    "                audio = resampler(audio)\n",
    "\n",
    "            \n",
    "            # add audio to sample\n",
    "            sample.append(audio)\n",
    "\n",
    "        # apply transform if necessary\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'commonvoice\\\\cv-corpus-16.0-delta-2023-12-06\\\\de\\\\validated.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SpeechDataset(tsvs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommonvoice\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcv-corpus-16.0-delta-2023-12-06\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124men\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvalidated.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommonvoice\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcv-corpus-16.0-delta-2023-12-06\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mde\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvalidated.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommonvoice\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcv-corpus-16.0-delta-2023-12-06\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mja\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvalidated.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset length:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset columns:\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mget_column_names())\n",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m, in \u001b[0;36mSpeechDataset.__init__\u001b[1;34m(self, tsvs, sample_rate, transform, columns)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# load metadata\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metadata()\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mSpeechDataset._load_metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m clips \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclips\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# read tsv and append to data\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tsv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     32\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# commonvoice columns:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# client_id\tpath\tsentence\tup_votes\tdown_votes\tage\tgender\taccents\tvariant\tlocale\tsegment\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \n\u001b[0;32m     37\u001b[0m         \u001b[38;5;66;03m# get columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ben\\.conda\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'commonvoice\\\\cv-corpus-16.0-delta-2023-12-06\\\\de\\\\validated.tsv'"
     ]
    }
   ],
   "source": [
    "dataset = SpeechDataset(tsvs=[\n",
    "    'commonvoice\\\\cv-corpus-16.0-delta-2023-12-06\\\\en\\\\validated.tsv',\n",
    "    'commonvoice\\\\cv-corpus-16.0-delta-2023-12-06\\\\de\\\\validated.tsv', \n",
    "    'commonvoice\\\\cv-corpus-16.0-delta-2023-12-06\\\\ja\\\\validated.tsv'], columns=['path', 'sentence'])\n",
    "\n",
    "print('Dataset length:', len(dataset))\n",
    "print('Dataset columns:', dataset.get_column_names())\n",
    "\n",
    "import random\n",
    "# get first sample\n",
    "sample = dataset[random.randint(0, len(dataset))]\n",
    "print('Sample:', sample)\n",
    "\n",
    "# get audio from sample\n",
    "audio = sample[-1]\n",
    "\n",
    "# play audio\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Play the audio using IPython's Audio widget\n",
    "audio_widget = Audio(data=audio.numpy(), rate=16000)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# baseline models\n",
    "import torch.nn as nn\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def pad_batch(batch):\n",
    "    # pads batch to longest sequence\n",
    "    # batch is list of samples\n",
    "    lengths = [len(sample) for sample in batch]\n",
    "    max_length = max(lengths)\n",
    "    # pad to max length\n",
    "    padded_batch = [torch.nn.functional.pad(sample, (0, max_length - len(sample))) for sample in batch]\n",
    "    return padded_batch\n",
    "\n",
    "class BaselineEmbedder(nn.Module):\n",
    "    def __init__(self, sample_rate = SAMPLE_RATE, embedding_dim=32):\n",
    "        super(BaselineEmbedder, self).__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=embedding_dim, num_layers=3, batch_first=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is audio, clips are padded to longest sequence\n",
    "        # x is (batch_size, samples)\n",
    "\n",
    "        # reshape to (batch_size, samples, 1)\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.lstm(x)\n",
    "        # get last hidden state\n",
    "        x = x[0][:, -1, :]\n",
    "        x = x.reshape(-1, self.embedding_dim)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineEmbedder(\n",
      "  (lstm): LSTM(1, 32, num_layers=3, batch_first=True)\n",
      ")\n",
      "Input shape: torch.Size([16, 122112])\n",
      "Embeddings shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "baseline = BaselineEmbedder()\n",
    "print(baseline)\n",
    "\n",
    "batch = [dataset[random.randint(0, len(dataset))][-1] for _ in range(16)]\n",
    "batch = [sample[-1] for sample in batch]\n",
    "batch = pad_batch(batch)\n",
    "batch = torch.stack(batch)\n",
    "\n",
    "print('Input shape:', batch.shape)\n",
    "\n",
    "# get embeddings\n",
    "embeddings = baseline(batch)\n",
    "print('Embeddings shape:', embeddings.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
